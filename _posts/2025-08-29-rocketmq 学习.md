---
title: rocketmq 学习
author: 刘Sir
date: 2025-08-28 10:10:00 +0800
categories: [技术]
tags: [redis]
render_with_liquid: false
---

## 顺序消息保证
1. 订单/交易系统：用户下单 → 扣减库存 → 生成支付记录。同一业务相关的消息，可以保证按发送顺序被消费
2. 高可靠性： 交易系统不能丢消息，
    -  消息写入磁盘才算成功
    -  Master-Slave 或多副本复制，不丢消息
    -  事务消息：可以保证消息与业务操作一致性（事务半消息 + commit/rollback）
3. 延时消息/定时消息
    - 支付超时自动关闭订单
    - 延迟通知用户或库存恢复
4. 高吞吐 & 高可用
    - RocketMQ 设计基于 分区队列 + 异步复制
    - 支持高吞吐量的消息写入和消费
    - Master-Slave 自动切换
5. 事物消息（解决本地事务与消息不一致的问题）
    - 下单 → 扣库存 → 生成支付记录，需要原子性
    - Producer 发送半消息
    - 根据业务结果提交或回滚消息

## 事物消息
https://rocketmq.apache.org/docs/featureBehavior/04transactionmessage
**mq 的事务消息只是保证本地事物和消息发送一起成功，或者一起失败，并不是跨服务的分布式事物**
在使用 mq 的过程中，生产者和 mq 之间是通过网络传递的，网络传递就具有不完全可靠性。比如有很重要的消息一定要发送到 mq 中让其可以消费，但是因为网络我们没办法保证我们的消息一定能投递到 mq 中，比如：我们业务下单后需要投递消息到 mq 的列子，业务代码可以保证本地事物（下单）成功，但无法保证一定能投递到 mq。以往我们要解决这个问题需要业务记录投递记录表，定时扫描 + 人工补偿。那么现在 rocket mq 通过这样一个事物机制帮助我们实现了自动处理这个过程。其实现的方式关键是有 3 个地方：
1. 消息半提交：生产者第一次和 mq 通信是 half-messge （ mq 不会投递到消费 ）
2. 本地事物：本地事物提交成功/失败 告诉 mq，commit/rollback
3. 事物回查机制：在 2 的场景下，也可能发生网络问题，mq 就会通过回查生产者的事物状态（间隔时间，回查次数）
本地事物的状态只用业务自己清楚，因此我们需要增加一个回调（callback），监听（listener）也好让 mq 回查的时返回对应的事物状态。其实这里还是无法完全保证，因为如果超过回查的次数就可能不会回查了，half message 会被丢弃。因此为了更多的保证像这种场景下业务方还是需要记录一个事物消息状态表进行兜底，另外要监控 mq 的日志中 half-message 丢弃的日志，及时感知异常消息。

## tcp 拥塞控制
网络中的路由器、链路等资源被过多的数据占满，导致 丢包、延迟增加、带宽利用率下降，tcp 处理拥塞的方式：
- 慢启动：利用拥塞窗口 vwnd，初始窗口 cwnd=1mss，小于 ssthresh 指数增长。（很像慢慢试探，慢慢加大窗口值。而不是一开始就冲）
- 拥塞避免：当 cwnd >= ssthresh，到达这个阈值后，变成线性增长，避免速度过快
- 快重传：三个重复的 ACK，不等超时，立刻重传丢失的包
- 快恢复：丢包后，重新评估 cwnd 的值，降低速度
就是根据 tcp 链路中的实时状态（丢包，ack，延迟）等情况，实时调整链路的速度。

## rocketmq 的组件和作用
- broker：核心组件，负责消息的存储（commitlog + 索引），转发，过滤，回查。
- nameserver：服务发现和路由管理，无状态的，多节点部署，互不通信，保证高可用
- controller（5.0 版本开始引入）：负责Broker 主从选举 和 元数据管理，raft 协议，3 个节点以上（kafka zookeeper）
- proxy：5.0 后引入，在 broker 和 客户端中增加的一层，多租户和 Serverless 场景

可以看出 rocket mq 组件多，架构较为复杂。从 5.0 版本后引入 controller 解决主从自动切换，额外再引入 proxy 组件放到 broker 前，屏蔽 broker 细节，使得 rocket mq 架构更复杂。但是总体的高可用还是基于 broker 的主从复制（异步和同步），实际上在我的角度看来既然引入了 proxy 在前，那么 broker 的主从架构其实可以变成多主架构，数据在主主之间复制整体的架构会更简洁。这里可能是基于历史的原因：RocketMQ 的底层存储引擎是 CommitLog + ConsumeQueue，它本质是“单点顺序写”设计，多主写会破坏这个模型。要改的话几乎等于重写存储层。 另外 RocketMQ：追求 事务一致性、低延迟、金融级可靠，不容许随意丢失/乱序

## 顺序消息
这个特性不难理解，难理解的是为何 rocket mq 单独拿出来说。rabbitmq 和 kafka 其实也都支持顺序消息，
- rabbitmq：单个队列就说严格 fifo 机制，需要配合生产和消费者一起实现
- kafka： Partition 内严格 FIFO，指定 key 保证同 key 的消息顺序，但跨 Partition 无序。
- rocketmq：通过 MessageQueueSelector（生产端）+ MessageListenerOrderly（消费端）直接帮你封装好顺序逻辑

因此现在看来大家都支持，只是支持的方式不一样，rabbitmq 和 kafka 需要你在业务代码上做更多的处理，但是 rocketmq 直接集成在 sdk 中，开箱即用。同样可以得知为了保证消息的顺序性，因此在某种程度上需要牺牲一定吞吐性能。 在实际的开放中也可以使用业务逻辑的方式控制顺序，不依赖 mq 的顺序，这样或许在某些场景中可以提高业务的吞吐性能。


## rabbitmq vs kafka vs rocketmq
| 维度              | RabbitMQ                                   | Kafka                                   | RocketMQ                                                      |
| --------------- | ------------------------------------------ | --------------------------------------- | ------------------------------------------------------------- |
| **定位**          | 通用消息队列，强调路由灵活性、易用性                         | 分布式日志系统，强调高吞吐、流处理                       | 企业级分布式消息平台（消息 + 事件 + 流）                                       |
| **协议支持**        | AMQP 0.9/1.0, MQTT, STOMP, HTTP, WebSocket | 自定义 TCP 协议                              | 自定义协议 + 支持 OpenMessaging，扩展 EventBridge、MQTT、gRPC             |
| **消息模型**        | Exchange（直连、topic、fanout、headers）+ Queue   | 主题（Topic）+ 分区（Partition）                | 主题（Topic）+ 队列（MessageQueue）+ Tag + Key                        |
| **顺序消息**        | 每个 Queue 内 FIFO（严格顺序需单 consumer）           | 分区内 FIFO（多分区打乱全局顺序）                     | 提供 **MessageListenerOrderly** 开箱即用的顺序消费                       |
| **延迟消息 / 定时消息** | 无原生支持（需插件或死信队列实现）                          | 无原生支持（需定制，通常依赖流处理框架）                    | 原生支持（延迟级别 / 定时投递）                                             |
| **事务消息**        | 支持（但较重，吞吐下降大）                              | 无（只能通过幂等 + Exactly Once 语义实现类似效果）       | 原生事务消息（两阶段提交，适合金融、电商）                                         |
| **存储机制**        | 内存优先，磁盘持久化可选                               | 顺序写磁盘 + 页缓存，保留完整日志                      | CommitLog 顺序写，ConsumeQueue 索引加速消费                             |
| **消息存储时长**      | 默认消息消费后删除                                  | 可配置，常见保存 7 天或永久                         | 可配置，默认 3 天（支持持久化扩展）                                           |
| **高可用机制**       | 镜像队列 / Quorum Queue（Raft 共识）               | 多副本（ISR 副本集，依赖 Raft-like 共识）            | 多副本（主从 + DLedger Raft 模式）                                     |
| **性能**          | 万级 QPS，低延迟（ms 级），适合业务消息                    | 十万 \~ 百万级 QPS，低延迟（ms \~ 百 ms），适合日志流     | 十万级 QPS，延迟低（ms 级），兼顾事务和延迟消息                                   |
| **典型场景**        | 异步解耦、任务队列、IoT 消息推送、跨语言集成                   | 日志采集、数据管道、实时流计算、监控埋点                    | 电商交易、金融支付、分布式事务、延时任务                                          |
| **运维复杂度**       | 部署简单，但插件多、集群大时复杂                           | 运维 Kafka + ZooKeeper（新版本 KRaft 简化）      | 组件多（Broker, NameServer, Controller, Proxy…），比 Rabbit/Kafka 复杂 |
| **生态**          | 插件丰富，支持多协议（适合异构系统集成）                       | 生态强大，和 Flink、Spark、ClickHouse 等大数据体系紧耦合 | 阿里系生态，支持 EventBridge、Connect、MQTT，向消息平台发展                     |

因此总结：
RabbitMQ：轻量级，支持多种协议，路由能力最灵活，适合 异步解耦、任务驱动，但吞吐一般。

Kafka：高吞吐、低延迟，定位就是 分布式日志系统 / 流处理基石，不适合作为通用 MQ。

RocketMQ：在功能上更全，天然支持 事务消息、延时消息、顺序消费，偏向 企业级业务场景，但运维复杂度较高。