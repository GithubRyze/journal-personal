---
title: 面试训练-逻辑题
author: 刘Sir
date: 2025-08-13 10:10:00 +0800
categories: [生活]
tags: [随记]
render_with_liquid: false
---


### Java 和 Golang 开发后端服务，请问你认为这两种语言在实际项目开发中各自的优势和劣势是什么？你在什么场景下会优先选择 Golang？

java：动态语言，启动慢，生态要比 go 好，占用内存大，适合大型的系统选择，如 erp，金融，电商等 
go：编译语言，启动快，内存占用比较 java 小很多，内置网络库开发网络应用非常快，好。另外协程模式对比 java 的线程模式，在执行效率上要高于 java，更适合轻量化、高并发服务

### 你提到 Go 的协程在执行效率上更好，能不能说说 Golang 的协程（goroutine）是如何实现轻量化的？以及它和线程在调度机制上最大的差别是什么？
协程：应用层，用户可在应用层控制协程执行和退出，调度依赖用户的代码控制，协程的切换代价较低
- Goroutine 初始只占用 2KB 栈内存（线程通常是 1MB）
- 栈是可以动态扩容的，因此可以成千上万并发不爆内存
- 不依赖 OS 调度，而是由 Go 的 runtime 执行调度逻辑（GMP模型）

Go 的协程是通过 M:N调度模型 实现的，其中：
- M（Machine）：对应内核线程
- G（Goroutine）：Go 协程
- P（Processor）：执行上下文/调度器，一般与 CPU 核心数绑定
调度器负责把 G 分发给 M 执行，类似协程跑在线程上的调度，避免了频繁系统调用，提高了切换效率。

线程：系统层，线程的执行和退出，都依赖于系统 cpu 的调度，一般是抢占式执行，系统切换线程的代价高。

因此Go 在高并发和 IO 密集场景下有明显优势

### 你在项目中使用了 Redis、RabbitMQ、RocketMQ 等中间件。请你从使用和特性角度对比一下 RabbitMQ 和 RocketMQ，它们各自适用于哪些场景？你自己倾向使用哪一个，为什么？
在我参与的项目中，RabbitMQ 用得比较多，主要原因是它对消息可靠性支持比较全面，比如 TTL、DLX、消息确认机制（ack）、多种交换机类型（直连、广播、主题等），在需要灵活路由、业务解耦时非常合适。

如果是对吞吐量要求极高、消息堆积量很大的场景，比如日志采集、订单流转，我更倾向选 RocketMQ，因为它是为高吞吐设计的，底层用的是 commitlog+ConsumeQueue 的结构，支持百万级 TPS，且提供了较好的可观测性。

RabbitMQ 更适合中小型系统、灵活的业务流程，而 RocketMQ 更偏向于大流量、高可用、高性能场景。我如果负责选型，会结合系统的消息量、实时性要求、运维能力等做判断。

我没有对比他们的特性，rabbitmq 我在选择的时候看到其支持 TTL（Time-To-Live）超时处理，DLX（死信队列）等特性，如果需要我去选型的话 我会仔细对比他们的特性，比如：持久化，高可用，消息模型，消费模型pull/push 等这些方面结合我方系统的需求去选

#### 为何不一开始就选择 rocketmq，避免后续升级/迁移
- RabbitMQ 社区活跃、文档完善、客户端兼容性强，适合快速上手和开发调试
- RocketMQ 虽然强大，但学习曲线和配置细节要复杂一些（尤其是运维监控部分）
架构选型其实就是一个平衡问题，不太推荐一上来就“插到底”，因为任何一项技术的引入，都伴随着人力成本、学习成本和运维成本。
一个大型系统的高可用、高性能，往往不是一蹴而就，而是伴随着业务增长逐步演进的结果。我们会在当前阶段尽量用最小的投入获得足够的可靠性，而不是一开始就用最复杂的方案。
所以像 MQ 这类组件，我倾向于先以 RabbitMQ 这类“轻量可控”方案落地，等发现真正瓶颈，再考虑 RocketMQ 或更高性能的替代方案。

### 你提到 K8S 的部署和故障排查经验，请问你平时怎么排查一个服务部署到 K8S 上后，外部访问不了的问题？可以大致说下你的排查思路或步骤
这种情况下遇到过，大部分是两个问题导致的：
1. 网络问题
2. 配置问题
这个首先我们需要清楚在访问 k8s 中的服务过程，大致过程如下：前端请求 -> ingress -> nginx 前端代理 -> k8s service -> k8s pod
我们遇到最多是 nginx 代理写错，或者 k8s 服务没有绑定 pod 的情况。出现这个问题可以先看 nginx 前端代理 的日志/或者 主机到 ingress 的网络（域名，dns，端口等），因此排查的重点是：
- pod 的状态，是否正常运行并且和service 绑定
- ngixn代理配置和日记
- ingress 的配置和日志

### 你用 Golang 开发服务时，有哪些常见的并发模型或模式？你有没有遇到过由于并发导致的 Bug 或资源争用问题？你是怎么解决的？
项目中遇到多是 生产-消费，协程池/线程池，资源锁，异步 等，遇到过因为没有加锁导致变量被其他线程/协程 修改导致不符合预期，这种情况下加锁即可，异步操作 在短信网关项目中，短信的 ack 是异步返回，导致数据还没保存回执已经返回找不到原数据，这种通过梳理代码逻辑，增加redis缓存可解，

### 你在使用 MySQL / PostgreSQL 时有没有做过数据表的设计？请说说你在设计数据库表时，通常会考虑哪些因素，如何权衡规范化与性能之间的关系？
有时候为了查询的效率我们考虑沉于一些字段，设计字段的时候的类型和索引等，避免但表数据大可提前根据业务进行分表。
尽量规范设计，性能第二，因为系统的性能可以通过提升硬件的方式提升，但是设计好个数据表可以提升系统的稳定性，可维护性，这些后面很难通过其他的努力提升。

### 你在项目中搭建过日志采集方案，提到了 Loki / Elasticsearch。请你对比一下它们的主要差异，为什么后期选择 Loki？它有什么优势？
可能是我简历让你们误会了，我们并没有使用  loki 代替 es。
loki：不对数据内容进行特殊处理，只是一直追加数据到磁盘中，当然它会根据配置合并数据，其主要是靠 label 标签进行索引，加快查询速度，它不太适合对日志进行聚合查询，评分，但是它占用内存小，能满足很多简单日志存储和查询的需求，在我影响中高可用方式好像支持的不太好
es：支持高可用，可存储日志，指标，并且提供的各种不同的检索方式（聚合，分词，评分等），它占用内存大，适合中大型系统日志的存储和检索，并且是 json 的数据结构。

### 你在短信平台中使用了 TCP 长连接处理下游的回执推送。如果大量连接同时存在，系统资源会吃紧。请问你是如何管理这些连接的？是否遇到过粘包/拆包问题？你是怎么处理的？

tcp 管理： 缓存连接，通过心跳检测死链接，很长时间没有数据通信主动关闭链接，设置合理的系统 keeplive 值，因为我们是底层基于 netty 处理数据的拆包和粘包，具体我忘记底层怎么处理。但是处理不外乎：
- 基于消息长度：比如 消息头，消息体。固定读取消息头解析内容，再读消息体。这种基于消息长度处理拆/粘包
- 基于消息结束符，可以约定固定的消息结束符 比如 /n

### 如果 TCP 数据读取时出现了一个包头说 body 长度是 512 字节，但底层只收到了 500 字节，你的处理机制是怎样的？
一般在接收的时候我们需要配置缓冲buffer和消息不完整超时，运行过程中记录日志，丢弃。
然后排查原因：消息发送是否发送完成，接收消息是否从底层 flush 到了应用层。
网络中间会丢消息 - 可以抓包

### 你在项目中是否遇到过协程泄漏的问题？请你描述一次具体的情况，是怎么发现的，又是如何解决的？
有次客户告诉我们一个台机器上内存使用过多，这个机器部署了一套系统，但是基本没有什么访问量，因此我远程使用 podman 命令看到其中一个内存占用非常大，判断上有内存泄露，这个系统不上我处理的，我只负责部署。然后这个同事一直在排查代码，并且我也告诉他怎么去排查 ，但是问题没有解决。后面我拉下他的代码，检查代码发现 grpc 客户端没有复用，因此我运行他的代码，然后通过 6060 端口，间隔不同时间去抓去内存消息，发现这个 grpc 的对象数量一直在持续增长，最终修改代码复用 grpc 客户端， golang 中很多这种地方需要注意，不像 java spring 可以管理这些对象，go中都需要手动代码管理
- 开启 6060 监听
- 浏览器访问 http://<host>:6060/debug/pprof/heap

Alloc Objects 和 In Use Objects 是否持续上升
goroutine profile 中 grpc 相关的调用栈是否越来越多（比如 grpc.NewClient，或者 grpc 的连接调度线程）

### 如果你的服务要向多个下游地址发 grpc 请求（例如几十个），你会如何设计 client 的复用机制？
连接池，链接池中用 map 来保存
- 设置合理的池大小，
- 设置唯一的客户端 id，缓存到 map中
- 设置锁防止创建同一个 client
- 可以设置指标，客户端数量，响应时间等进行监控
- 设置淘汰策略，清理僵尸链接，grpc keepalive 配置，os 层面也 tcp 的socket 配置，
- 设置熔断策略，针对链接失败/超时的可以尝试重连，记录失败次数。响应慢的可以记录日志，告警

### 请你设计一个高并发场景下的分布式限流系统，并说明你的设计思路、关键技术选型和可能遇到的问题，以及你如何解决这些问题。
#### 限流策略配置
支持按用户、API、服务、IP 等维度配置限流规则
规则动态下发，支持实时修改
#### 限流算法选择
常用算法：计数器、漏桶、令牌桶、滑动窗口计数等
不同算法适合不同场景，如瞬时流量峰值防护、平滑限流等
- 计数器限流 Redis INCR 计数器， key：api_name:user_id:yyyyMMddHHmm： 可能存在瞬时请求过大（1 分钟的最后一秒请求非常多，接下来新一分钟又允许 100 次）
- 滑动窗口限流（Sliding Window，请求时间戳存入 Redis 的有序集合（ZSET），需要存储每个请求的时间戳，数据量大时有存储压力，需要定期清理过期数据，增加 Redis 负载
- 令牌桶算法（Token Bucket），以固定速率往桶里放令牌，拿到令牌则通行。Lua 脚本原子更新令牌数量和时间戳
- 漏桶算法（Leaky Bucket），计数请求数量，控制处理请求的数量（以额定的速率流出），
#### 分布式实现
集中式存储限流状态（Redis常用，支持原子操作）
保证分布式环境中限流状态的一致性和性能
处理多节点间的同步问题，避免重复放行或错误限流
#### 性能与容错
限流操作需快速执行，尽量减少网络和存储延迟
容错设计，如限流系统不可用时的降级方案
- 多实例部署
- 在应用服务中，增加默认限流策略（当 依赖的 redis 限流系统无法使用，可启用，做到系统继续对外提供服务）
#### 监控限流系统健康，及时报警


# java 基础和进阶
## java中的内存模型（JMM）是什么？它解决了什么问题？
Java内存模型（Java Memory Model，JMM）定义了Java程序中线程如何与内存交互，尤其是主内存和工作内存之间的关系。它解决了多线程环境下变量的可见性、有序性和原子性问题，规范了并发访问共享变量时的行为，保证线程安全。
## volatile关键字的作用及使用场景？
volatile保证变量的可见性，即一个线程修改了变量，其他线程立即能看到。同时防止指令重排序。适用于状态标识量等场景，但不能保证复合操作的原子性。
## Java的垃圾回收机制是如何工作的？常见的垃圾收集器有哪些？
JVM自动回收无用对象的内存，主要通过标记-清除、复制、标记-整理等算法。常见GC有Serial、Parallel、CMS、G1等，分别适用于不同的场景和性能需求。

## synchronized和ReentrantLock的区别？什么时候用哪种？
synchronized是Java关键字，内置锁，自动释放；ReentrantLock是API，功能更丰富（支持公平锁、可中断、超时尝试锁等）。需要高级锁功能时用ReentrantLock，否则用synchronized简洁
##  线程池的核心参数有哪些？
核心参数包括：核心线程数（corePoolSize）、最大线程数（maximumPoolSize）、线程空闲时间（keepAliveTime）、任务队列（workQueue）、线程工厂、拒绝策略
## Java 8的Stream API解决了什么问题？优势？
Stream API提供声明式处理集合数据的方式，支持链式操作和并行执行。相比传统循环，代码更简洁、易读且可利用多核并行处理提高性能。
## Spring Boot的启动流程？
Spring Boot通过SpringApplication.run()启动，加载Spring上下文，自动装配配置类，启动嵌入式容器，完成应用初始化。
## Spring中Bean生命周期关键步骤？
实例化、依赖注入、初始化（调用初始化方法或InitializingBean接口）、使用、销毁（调用销毁方法或DisposableBean接口）。
## 理解Spring的依赖注入（DI）和控制反转（IOC）？
IOC是设计原则，控制对象创建和依赖关系由容器管理
- 核心是将对象的控制权从程序代码中“反转”出来，交给容器或框架来管理。
传统方式：
- 应用代码主动创建和管理依赖对象，比如在类中使用 new 操作符自己实例化依赖。
- 传统开发中，程序自己控制对象创建和生命周期。
IOC方式：
- 应用代码不再主动创建依赖对象，依赖由外部容器负责创建和注入，应用只使用对象。 
- 控制权被“反转”，由框架或容器控制对象的创建和生命周期。
- 解耦：业务逻辑代码不依赖于具体的实现，依赖通过配置或注解注入，增强灵活性。
- 易测试：方便替换依赖实现进行单元测试。
- 增强扩展性：可根据配置或环境替换不同实现。
DI是IOC的具体实现方式，通过注入依赖对象来实现组件解耦。
## DI（依赖注入）如何处理循环依赖？
Spring 通过 三级缓存机制 来解决 单例 Bean 的构造器循环依赖，
| 级别   | 缓存名                     | 缓存内容                            |
| ---- | ----------------------- | ------------------------------- |
| 1级缓存 | `singletonObjects`      | 完成初始化的 Bean 实例                  |
| 2级缓存 | `earlySingletonObjects` | 只完成实例化、但未填充属性的 Bean             |
| 3级缓存 | `singletonFactories`    | 用于生成 2级缓存中 Bean 的 ObjectFactory |
1. Spring 开始创建 A（A 依赖 B）
2. A 实例化完成，放入 3级缓存（对象工厂）
3. Spring 发现 A 依赖 B，于是开始创建 B
4. B 实例化完成，也放入 3级缓存
5. B 发现自己依赖 A，就从缓存中尝试获取 A（A 还没初始化完成，但已经有早期引用，于是从 3级缓存中通过 ObjectFactory 拿到早期 A 放入 2级缓存）
7. B 拿到 A 的早期引用，注入成功
8. B 完成初始化，放入 1级缓存
9. 回过头继续初始化 A，注入 B，A 也完成，放入 1级缓存

## HashMap的工作原理及线程安全问题？
HashMap通过数组+链表/红黑树实现，利用hash值定位桶，支持快速查找。非线程安全，多个线程并发修改可能导致数据不一致或死循环。
## 什么是死锁？怎么避免？
死锁是两个或多个线程互相等待对方释放资源，导致永久阻塞。
- 尽可能的避免竞态执行，可以采用排队方式
- 设置锁的超时时间
- 避免互相嵌套锁的场景
## Java中ClassLoader是什么？作用？
ClassLoader负责加载字节码文件（.class）到JVM内存中，支持不同的加载机制（父子委托、双亲委托），实现动态加载类。
## JVM调优中常见的性能瓶颈和排查方法？
瓶颈包括GC频繁、内存泄漏、线程阻塞等。排查方法有使用jstat、jmap、jstack等工具分析堆内存、线程状态和GC日志，定位问题。


## 你提到参与短信网关的开发，请描述一次从接收到短信请求到下发的完整流程？在这个过程中存在哪些高并发或异步处理的挑战？如何解决？
大致流程：http/tcp 接收短信 -> 过滤（敏感词，黑明单）-> 找路由（短信不同地区有不同的供应商） -> 计算短信条数（长短信在系统中是2条）和扣费 -> 存储记录并且发送给第三方供应商 -> 收取短信 ack -> 返回 ack 给客户端。

因我们的架构上：收单，处理短信逻辑，发送短信和ack。高并发的地方在处理短信的业务逻辑处。为什么不说收单呢，因为我们支持批量收单，客户端也很少单条发送，因此在收单后我们会拆包，然后逐条发送给短信业务处理系统，业务处理系统对每条短信都要经过，过滤，路由，计数，扣费 等操作，另外发送供应商服务在我们需要单条发送，并且这里需要记录短信内容，短信唯一号，异步处理 ack。 异步处理的挑战主要是 ack 返回，可能返回的快，可能很慢（），还有一个特点是供应商有时候的处理能力不是稳定的，可能快，可能慢。

针对这些情况，我们在系统之间引入了 rabbitmq 消息队列作为缓存层，并且配置重发队列。引入 redis 来缓存 我们的短信主键（第三方供应商要求的主键和我们的不一样，因此我们在发送的时候需要构建一个满足第三方的主键），然后通过第三方的主键反向获取客户的短信

### 你提到使用 Redis 来缓存主键映射，如何避免 Redis 中缓存数据丢失影响 ack 的处理？
当然配置了实效时间，否则redis 会越来越大，如果 ack 在长时间没有回复，在后面收到的时候，匹配不到 redis，我们会单独进行记录，另外在系统中短信没有收到 ack 会有通知，客户服务会跟进或者手动标记发送状态等。
### ack 如果非常快地到来，Redis 还没写完怎么办？有没有 race condition 的风险？
我们先写redis，在发送，没有这个情况
### RabbitMQ 中，如果消息积压严重，或者消费速度赶不上，系统会如何应对？是否做了限流保护？
当时我们没有处理。
现在我想可以
1. 增加 mq 的指标监控，实时获取 mq 的状态。
2. 增加限流策略，初步考虑令牌桶的策略，因为不同的供应商处理能力不一样
3. 或者可以把不同的供应商服务单独独立出来，根据不同的处理能力，配置路由策略

### 你们系统对“错误 ack”的处理方式是怎样的？比如 ack 来了，但内容有问题，或者是伪造的？
这个当时也没有考虑到，现实中也没有发现。如果遇到我们可以 日志记录，然后丢弃，实际上伪造的情况不太存在，因为第三方供应商 我们的链接都是需要用户认证的，如果需要我们设计一个 ack，增加签名校验机制

### 短信重新发送
客户端每条短信都有唯一识别吧，我们也会给短信增加额外的一条网关唯一识别，使用这个跟踪这条短信在网关的流动

### 你提到 Redis 用来存储 ack 对应关系，如果 Redis 崩了，会对系统有啥影响？有没有降级策略？
这个当时因为时间和设计经验上没有做出应对，现在让我来优化这一点我会这样处理：
redis crash 我们系统几乎不可用了，所以redis 短信网关来说非常重要，之前说单个 redis，现在我们可以把 redis 做成高可用，或者再增加一个 数据库的二级缓存，当redis 不可用，我们去读数据库。读数据库的话 需要配置合理的限流策略，另外我们可以增加监控，增加限流策略如：令牌桶/漏桶算法对访问速率做控制

### 你们如何处理长短信拆分和发送的幂等性问题？比如同一条长短信多次接收，如何避免发送多次？
我们只会在计数和扣费的时候拆分但是数据层面还是单条，然后在发送供应商的时候 会进行长短信拆分，具体的拆分规则我忘记了

### 你们系统中短信处理流程是异步的，如果 MQ（RabbitMQ）临时不可用，比如节点挂了或者网络中断，系统是如何恢复的？是否存在消息丢失的风险？你当时是怎么处理的？现在你会如何改进？
之前  mq 不可用，基本我们的业务不可用，我们开启了消息持久化机制防止消息丢失，另外我记得 rb mq 有消息确认机制，
如果说现在：
- rbmq 增加高可用
- 监控 rbmq
- 应用系统中对 mq 进行熔断和降级处理，编码在mq 不可用时，通过 restful 或者 grpc  进行服务之间的通信。


### 设计一个通用的“收费项目处理”流程，保证扩展性和一致性，你怎么设计？（流程和架构）

#### 收费项目配置
此处特点是写少读多，需要考虑查询效率问题，一般的 sql 数据库可以满足 （oracle，mysql，pgsql），考虑数据库读写分离，高可用模式

#### 账单系统
- 收费项目需要设计账单中的唯一号
- 第一次采用主动 grpc，dubbo 通信方式主动提交
- 费用明细的修改可以采用  mq ，高可用（生产和消费模型）
- 很多 his 可能需要考虑费用明细的可回溯，需要单独记录明细的修改历史记录 （这里可以单独表设计，不包含在账单中）
- 账单支付完成，部分支付
- 手动操作账单状态和配合 mq 更新状态状态 （比如 app， 自助机支付等）
- 账单的唯一号，账单数据的一致性。 重复提交账单，多人修改账单 （增加分布式锁，redis 哨兵和集群）
- 使用数据库事务与乐观锁结合防止脏写，保证账单数据的最终一致性。
- 关键业务操作设置幂等校验，避免重复执行带来影响
- 数据的审计日志，增加日志和指标等监控
### 如何保证医生在前端临时加项时，系统仍能保证计费的正确性，避免出现漏计、重复计费？
- 前端要做防抖，避免重复提交
- 每条收费明细都有设置唯一识别号
- 操作单个患者的账单时候，增加分布式锁
- 配合数据库事物和乐观锁结合防止脏写
- 定期对账（金额和明细是否一致，明细的数量，支付记录等对账）

#### 你提到定期批量校验，假如账单量非常大，这样的校验会不会对系统性能产生影响？你如何设计避免性能瓶颈？
会有一定的影响，所以对账的时间，最好是在系统比较闲的时候进行 ，比如凌晨医院的操作比较少。
- 分批进行
- 可以单独拆分对账系统，这样只要读就可以了，在 his oracle 我们没有分库和读写分离

#### 实时校验触发后如果发现异常，你说可以自动回滚或者标记异常状态，请详细说说这两种处理策略分别适合什么场景？如何实现？
自动回滚我们没有使用，都是通过状态标记，通知，然后人工核对。
#### 如果系统出现了分布式事务失败或网络抖动导致数据不一致，你会用什么技术手段来保证最终一致性？
记录失败日志，告警和通知，然后人工核对+人工补录

#### 医保结算流程中，如何保证费用与账单的一致性？
我们当时因为是香港业务不需要对接医保系统，但是需要对接 erp 和 香港医管局
- 对接 oracle， erp 我们是定时任务，晚上凌晨执行，同步进行的，对接会单独对账单进行记录，因为对接有一些额外的要求：比如拆单，医生费用和医院费用单独计费，支付信息（保险和普通支付也有区别），所以会有状态和同步记录一起
- 对接 ehrss 是后续处理的，是我们另外一个团队处理的，也是同步的 webservice 调用
- 对账失败的我们会有手动同步和告警+通知
实际上还是有优化的地方，比如可以单独设计一个对账平台或者billing 对第三方的适配服务

### 你提到账单系统需要保证账单号的唯一性和数据一致性，在高并发环境下，你是如何设计生成账单号的逻辑？有哪些潜在的问题？你是如何解决这些问题的？
我们当时是数据库 使用时 uuid, 账单业务号 由年月日 + 000001 的序列号组成（用户指定方式）
- 当前的潜在问题时 序列号需要每天刷新，序列号的产生是通过 oracle seq 产生的，产生不用的情况下不能回滚。
我们的解决方案是：
- 获取最新的账单号和解析当前日期进行对比，如果相同则直接获取序列号，不同则首先重置序列，再获取。这里会增加分布式
如果让我重新设计，我也不打算修改，因为够用，代价也很小
#### 你提到账单号由“年月日+序列号”组成，如果多台机器并发生成账单号，如何保证不重复？这个操作是否有性能瓶颈？
oracle 的 seq 本身属于原子操作，并且不会重复，性能肯定有，但是在医院的场景下账单的产生不属于高并发场景
#### 你说“凌晨重新归置 Oracle 序列号”，具体是怎么做的？有没有什么风险或 race condition？
获取最新的账单号和解析当前日期进行对比，如果相同则直接获取序列号，不同则首先重置序列，再获取。这步操作是假如分布式锁的，因此假如分布式不能用则账单号没办法产生。
#### 如果将来账单量进一步增大，单日内超过了序列上限（例如 99999），你准备如何扩展这套编号系统？
首先这个订单号是客户需要的，另外考虑到 his 系统的特点，单日订单不存在超过 999999 的账单，如果以后需要拓展，或者修改其他的方式。我们可以修改编号生成的函数即可，虽然我们这个编号有些不足，但是满足客户需求，而且无需额外依赖第三方服务，代价很小。

#### 账单状态设计时，有哪些常见的状态？这些状态变迁是否有状态机建模？
待支付，支付部分，支付完成，待退款，退款完成， 没有设计状态机建模
#### 如何保证账单状态变更的幂等性？比如消息重复、或者页面重复点击导致状态错乱怎么办？
- 账单和费用项唯一号
- web 防抖
- 数据库事物+乐观锁
- 分布式事务
- 费用明细操作流水 - 可回溯
通过上面这些措施可以避免消息重复和数据不一致的问题，如果出现了通过操作流水和审计日志，再经过人工处理
#### 手动修改账单状态时，如何避免并发写入（比如两个护士在不同终端同时操作同一个账单）？
- 账单和费用项唯一号
- web 防抖
- 数据库事物+乐观锁
- 分布式事务
- 费用明细操作流水 可回溯

#### 你提到费用明细的修改需要有历史记录，为什么要单独设计审计表而不是直接修改原表？
- 直接修改不可追溯
- 单独设计避免数据量大，审计的数据可以存储在 es 这些中
#### 审计表的设计上你会考虑哪些字段和结构？数据量大时如何做性能优化？
- 时间，操作人，患者，账单号，费用明细号 这些事最关键的
- 数据量大，可以放到 es 中
#### 在修改费用明细时，如何保证主表和审计表数据的一致性？分布式场景下如何保证事务？
- 这里依赖 oracle 的一致性事务，当时我们是没有分开的都是在同个库中
- 如果需要分库或者放到 es 中，这里涉及到数据一致性问题，可以这样处理：先提交主表，再写其他的库或者 es 中即可，如果考虑并发则可以采用 mq 最终一致性处理。另外增加对账功能，发现主表和审计不一致则通知，人工干预

#### 不同支付渠道（现金、银行卡、保险、移动支付）接口如何统一设计？
面向接口编程，抽取行为，让不同的支付方式去实现。工厂 + 策略模式
比如：支付行为，退款行为，账单状态查询。
另外可以在公共调用处，统一处理分布式锁，避免数据不一致，
统一处理错误和回调，保证业务流程一致性。


### 请介绍一下你主导设计的应用与数据连接平台的核心功能和整体技术架构。
核心功能
1. 反向代理功能
2. 任务执行流，通过组合不同的连接器（http，tcp，webservice，websocket，db，脚本等）完成任务。比如 前置http接收器，后置http client链接器，即可完成一次 http 请求。
4. 任务调度器
5. 日志和监控

技术架构
- 前端 react + grafana
- 中间逻辑 golang，gin，配置服务，任务调度服务，平台主服务，日志服务
- 存储 loki，prometheus，es，pgsql，minio，
- 集群日志采用 fluentbit + es
- 运行时 k8s + docker

### 你们平台支持 HTTP、gRPC、WebSocket 等多种协议，能具体说说你是如何设计协议转换的？在实现过程中遇到了哪些难点，又是如何解决的？
我们在接收和发送之间增加一个协议转换策略，可以通过配置的方式配置具体的协议转换规则。
- 不同协议之间的字段映射，还有数据嵌套的情况处理更复杂
这里我设计了一个 source 和 target 的数据结构的映射关系，然后策略中通过这个关系来进行转换。具体的实现是我同事负责的这一部分我只是参与设计，不负责具体编码，大致如下：
- 字段映射：原字段名，原字段类型，目标的字段名和字段类型，内置部分转换函数，支持string 转 int，日期格式等
- 嵌套对象：这里主要是结构化的数据结构 json 和 xml，配置中会配置路径表达，比如 json：user.address.home 这样，
- 异常处理：字段缺失，不符合配置，会有告警和通知
- 配置结构：采用 yaml，方便扩展
- 测试：在web 端，我们设计了测试按钮，可支持输入 json 点击发送，查看转换是否符合预期

grpc 过程：
- 反射服务允许客户端运行时获取服务和消息描述。
- dynamicpb 支持动态构造 protobuf 消息
- grpc.ClientConn.Invoke 可以用来动态调用 gRPC 方法


### 请详细描述一下你们平台的数据迁移功能是如何实现的？在保证数据可靠性和一致性方面做了哪些设计和措施？
也同样是在不同的数据中间增加了一个数据字段的转换映射层，把通过 sql 查询处的字段，映射到目标的数据库表中。这里配合任务调度器一起使用
- 对每次迁移都有监控日志和告警，通知等配合
- 支持触发器的方式，支持定时或手动触发数据迁移
- 支持存储过程
- 重试模式
- 手动触发/补偿 （失败的情况下）
实际代码中，我们目前针对 关系型数据采用的利用其内部的事务做到一致性，

此处应该回避问题数据迁移的部分，表面参与的是日志，指标，告警等部分的设计和实现
#### 你提到利用关系型数据库的事务保证一致性，那如果迁移过程涉及多个数据库，如何保证分布式事务或者跨库的数据一致性？
这个暂时没有考虑到，但是我们设计多个库的情况下，本身是通过任务执行流顺序执行，同步多个库可以配置多个数据库连接器

#### 当源数据发生变更，迁移任务正在执行，如何避免读到脏数据或造成数据不一致？
这个暂时没有考虑。在执行过程中数据已经从原数据查询出来了，已经避免不了。
#### 在数据映射层，复杂字段映射和类型转换是如何实现的？有没有遇到过性能瓶颈或实现难点？
复杂字段映射和类型 我们通过内置函数的方式实现，比如 string -> int 或者日期这种处理
#### 对于重试机制，如何设计幂等性，避免同一条数据重复迁移导致脏数据？
这个没有涉及到，我们在每次任务执行都会有唯一id，另外通过通过同步的日志记录 对比上次时间和目前的时间，防止重复执行
#### 你提到支持触发器和存储过程，这些数据库侧的逻辑如何与迁移任务的调度和错误处理结合起来？
这个没考虑过，实际好像也没实现，可能表达有问题，如果是你会怎么结合呢？
#### 数据迁移日志和告警系统是如何设计的？如何确保日志的完整性和可追溯性？
告警：会在应用用建立指标，比如执行耗时，执行成功/失败次数，正在执行的任务数量， counter，gauge，Histogram 等类型，通过 prometheus 库集成到应用中，http 接口暴露指标接口给 prometheus 采集。 prometheus 采用多实例部署，应用服务采用 statefulset 资源部署到 k8s 中，利用 statefulset 中headless和固定 pod 名称，配置多个 prometheus 去分别采集指标。 再结合 alert manager 进行配置告警和通知

日志：日志会记录此次任务的时间，唯一识别号，以及配置，执行状态等，全链路日志追踪

### 你提到任务调度模块支持多种连接器组合，比如 HTTP + WebSocket + DB 的串联任务，这里调度器如何协调异步任务之间的依赖关系？有没有任务失败后的重试、超时处理？
目前我们的调度模式实现比较单一，是按顺序执行，任务都是同步进行。当然我们后续也是打算增加异步调度的，引入 dag 的调度模型，异步依赖通过调度器配置，把入参和反参都记录到 上下文中。任务暂时没有设计失败重试和超时处理，但在设计日志和指标的时候已经记录了，并且可以告警+通知，在任务调度也有手动补偿执行。

#### 你说“同步串行执行”，那每个任务执行的上下文数据是如何在连接器之间传递的？有没有统一的上下文对象？能否动态计算参数？
我们有设计一个贯穿整个任务执行周期的 context 变量，连接器之间的参数目前是固定方式传入，连接器的执行结果就是下个连接器的执行参数。后续优化也会初步考虑通过 把参数配置到调度器中，在连接器中通过配置读/写 context 中参数变了实现
#### 如果某个连接器执行异常或超时，你们目前是“终止整条任务”，还是允许失败跳过？这个逻辑是固定写死的还是可配置的？
目前是终止整条任务，逻辑目前是写死的

#### 未来你打算支持 DAG 调度模型，是否调研过 Airflow、Argo 等项目？你打算自己实现 DAG 引擎，还是集成现有的？会遇到哪些挑战？
是的，有参考过这些项目，可能会选择自己实现，遇到问题：
- 性能
- 参数传递
- 日志跟踪
- 异步任务
- 调度模式 - 停止/跳过 

#### 你提到“连接器之间传参靠上一个结果作为下一个入参”，那如果出现类型不匹配、参数缺失，你们怎么处理？有通用校验机制吗？
参数缺失和类型不匹配，目前会终止任务执行，我们有设计一些参数的校验规则配置，比如字段值，格式等
#### 在 context 参数传递中，如果多个节点并发写同一个字段怎么办？你如何设计 context 的读写策略，是否是线程安全的？
目前我们没有这个情况，再后续异步任务执行会存在，这个需要增加锁，再调度器的实现中 我们已经设计同时只会调度一次
#### 你提到日志链路追踪，那你们有没有设计 TaskID、SpanID 类似的机制？如何做到链路可观察？
我们目前有类似的机制，在任务执行会有一个执行的唯一号，然后每个具体链接器执行都有一个单独的唯一号，日志中存储的是具体的链接器执行日志，执行的顺序我们也有记录，因此通过贯穿全任务的唯一号就可以做到追踪了

#### 你提到每个连接器有唯一 ID，那是否有统一的 trace viewer 或者图形化界面支持链路回溯？有没有使用如 OpenTelemetry、Jaeger 等工具集成？
我没有集成到现有的开源工具中，但是我们有自己的日志查询可视化页面可以查看，并且我们对不同第三方日志都有提供单独的平台，通过配置的方式实现了日志查看权限，即第三方 a 不能查看 第三方 b的日记，这在 his 系统中非常重要

#### 你说参数校验是“配置的”，能否介绍一下这部分配置的语法或引擎？是自己实现的 DSL 吗，还是用的 jsonschema / cel 这类工具？
是的主要是参考过别人的实现。数据结构大致如下
- json 和 xml 的字段路径
- 字段类型
- exrp 效应表达式
- 阈值

#### 未来异步调度后，任务重试或失败补偿是怎么设计的？任务失败后是否有 retry queue，或者人工干预机制？
失败重试主要需要考虑是单个连接器重试还是全任务重试，还是从失败的地方开始这些问题，以及有些任务可能不适合重试的问题。因此首先考虑
- 需要增加单链路手动重试的功能
- 重试的策略，需要配置重试次数，或者是否允许重试
- 执行失败目前使用告警 + 通知 + 日志的，
- 有机会会考虑采用重试队列和人工干预同时处理

#### 重试队列你们用什么技术栈做？自己实现的队列还是用 Redis/RabbitMQ？
考虑到我们系统特点重试队列我想应该不会太多，因此我打算之间使用数据表的方式实现，不再依赖第三方服务，引入更多的复杂性
#### 重试失败之后怎么避免数据污染？比如资金类任务是不是要有事务保障？
实际上我们的系统和任务执行流 还是区别的，我们更多是通过配置不同的连接器来完成协议转换，db 数据到 api，如资金累的数据保障，实际上我们系统并没有考虑事务的情形，设计系统的时候也没有把这个方面纳入我们需要支持的功能。我们平台会记录任务上下文与日志，帮助业务系统回溯或进行幂等判断。
#### 如果同一个任务在多个节点并发被调度怎么办？你们怎么做分布式调度锁？
这个调度器 我们是基于 pgsql select + update 的方式，间隔从 pgsql 中捞取任务调度，
FOR UPDATE SKIP LOCKED，status = 'pending' AND next_run <= now() 
因此这本身就是分布式的调度器。这也是我参考了很多定时任务系统模型后作出的选择，我的设计哲学是以尽可能最小的复杂度实现和满足功能即可。因为我们的人力本来就少，引入更多的复杂性会增加运维难度

#### 你们在做任务调度的时候，如果需要更新任务规则（比如修改 DAG、调整调度周期），是怎么在不停止调度器的情况下热更新的
目前这些功能还没有实现，目前严格意义上我们的 任务触发器，连接器调度，连接器 三个关系是，触发 -》连接器调度 -》连接器执行的过程，因此具体的执行和触发是分开两个服务的，因此我们需要按照下面这些作出调整：
1. 任务执行服务增加优雅退出，等待执行完成才能退出应用
2. 任务执行服务注册到触发器服务（掉线/在线），只会触发到在线的任务执行服务，再增加手动修改任务执行服务的状态。更新服务的时候直更新掉线的服务，让后再上线，任务执行服务多实例的情况下，依次按照这个逻辑替换

#### 在系统不重启的前提下，调度规则或任务配置更新后，能立即生效
目前我们就是这样做的，每次执行会从数据库捞起所有的配置，然后执行。因此在执行期间你更改了配置，不会印象正在执行的。下次执行就使用你最新的配置
#### 数据库查询所有配置比较耗时或者影响性能，你会考虑什么优化方案？比如缓存、增量更新之类的思路，能讲讲你的看法吗？
这个地方有考虑过，当前我们初步是这样设想的
1. 执行服务预加载所有的配置，每个配置都有唯一号和版本号
2. 任务配置变更，先落库，再通知每个执行服务更新响应的配置，如新增，更新，删除等
这样执行服务的内存会占用大一点，好处是没有第三方的依赖，简洁，另外如果后续演化也可以把配置做到 redis 中，过程和上面类似。只是执行服务直负责往redis 读，更新redis都有配置服务负责

#### 你刚才提到系统中有日志和告警机制，请问你们是如何设计任务执行的监控指标的？主要监控哪些关键指标？这些指标是怎么采集和存储的？有用哪些开源组件吗？
连接器调度指标：耗时，成功，失败次数，真正执行的数量
连接器执行指标：耗时，成功，失败次数，正在执行数量，进出流量

利用 prometheus 组件在代码中合适的位置写入指标，暴露 /metris 接口给 prometheus 抓取，
告警配置 alert manager + prometheus 进行告警和通知，后续我们目前也在自研告警，主要是 alert manager 高可用和 热更新配置不适合 web 操作和配置，目前我们已经开发了支持 prometheus 和 loki 数据源的自己告警机制，暂时在客户的 uat 环境测试

我们主要用的是 Prometheus 自带的 Alertmanager 规则，标准化告警配置比较成熟、稳定。

同时针对客户的特殊需求，我们也开发了自己的告警系统，支持更多自定义和灵活配置，目前在 UAT 环境测试，目的是提供更好的运维体验，比如基于 Loki 日志的告警等。

两者结合使用，保障告警的可靠和多样性。

#### 你刚提到自己研发的告警系统在 UAT 环境测试，能具体说说它相比 Alertmanager 有哪些优势或特点吗？未来计划怎么和 Prometheus 生态结合？
优势
- 配置更灵活，支持web yaml 方式热更新
- 可以支持更多的数据源（已经支持 prometheus 和 loki）
- 友好的交互 ui
- 可控性更好
劣势：
- 暂时没 alert manager 成熟，或者稳定
- 目前支持的功能也没 alertmanager 多
- 需要额外增加开发成本（调研，开发，测试等）

未来可以考虑支持 es，sql 等数据，另外我们有一个健康探测功能可以探测服务是否健康（http，tcp，grpc 已经支持），后续也结合到告警+通知体系，

#### 那你们有没有考虑告警的分级管理和责任人分配？比如不同等级的告警推送给不同的运维组，或者自动派单之类的？
告警分级和责任人，都可以在告警配置中以 label 的方式配置，通知的时候会像 alert manager 那样放到 json 中发送到通知方式，通知不同的组可以在配置 通知方式的时候 配置相应的 team 接收即可， web 上面目前只会显示告警记录和通知记录，当然我们web 有一列 会显示所有的 label 数据，这里其实也可以到你需要的分级管理和责任人，也支持检索

#### 那在告警通知渠道上，你们支持哪些方式？邮件、短信、钉钉、企业微信之类的吗？有没有做过通知的去重或频率控制？
自研的告警上面目前支持 飞书，邮件，webhook  三者，其他的待开发，但是我们web hook 是完全兼容 alert manager的 因此也可以通过 webhook 方式方式给 alert manage 让它帮我我们处理。 通知去重主要靠给告警规则去重，告警频率我们也有单独的配置，可以配置。通知本身不会有，主要通过控制告警来实现通知的去重和频率控制
#### 那告警的恢复通知你们是怎么做的？比如告警状态恢复时，是否也会通知相关人员？还是只在告警触发时通知？
目前这一块还没暂时支持，之前考虑了下支持起来也不复杂，主要是判断告警恢复的逻辑，然后在 json 里面修改状态值。
#### 你们系统中日志和监控数据存储分别用的什么？存储设计上有哪些考虑？如何保证性能和可扩展性？
日志： es loki
监控： promethues
存储主要是考虑高可用，当让 es 本身支持高可用，但是在日志方面需要管理 我是使用 es 的 stream + ilm 方式存储，查询的性能配置 hot，warm，cold规则，另外执行器的日志不是收集的方式，而是通过主动写入的方式。系统集群日志是 fluentbit 采集存储的。

prometheus 主要是高可用，但是它本身不支持高可用，在调研的时候有一种方案是 通过在 prometheus 集群前 放置一个 代理服务，代理服务负责把指标写入每个 prometheus 服务，我认为增加了复杂度，我不想采用这个方案。但是给我了一个方向，只要 prometheus 每个实例的数据不用追求一摸一样，本身就是指标只要在一个长时间维度上面差别不大即可。后面我就让多个 promtheus 去采集相同的服务指标就可以，因此后续我结合 k8s 的特点，把 prometheus 从 deployment 资源改成 statefulset 这样每个就一个单独固定的 k8s 地址，同时还需要把目标采集的 deployment 资源改成 statefulset 并且需要注入 pod name到应用中， prometheus 就可以根据 pod 名称去才采集多个实例。

#### 在日志和监控存储方案中，如何处理数据的容量增长和历史数据归档？你们做了哪些自动化手段？
集群日志配置 ilm 动态删除索引，压缩和合并的索引数据，只保留 15 的集群日志
连接器的执行日志，单独索引保存，目前不删除，只配置 ilm + hot，warm，cold 规则，压缩和合并的索引数据。
prometheus 我们配置了保留 15 的指标

#### 你提到 ES 里的 ILM（索引生命周期管理）规则，能详细讲讲你是怎么设计 hot/warm/cold 数据分层的吗？每层主要存储多久的数据？
具体的配置忘记，大约是大于多少天和多少 g，进入滚动，然后 7 天内是 hot，7-15 warm，大于 15 cold。 主要考虑查询 7 的日志是比较多的，实际上打大部分是查询 2 天内的日志，这里可以优化下。

#### 你们在日志和监控系统中有没有做过异常检测或者自动告警触发？比如日志量异常暴增或者关键指标阈值突破，系统是怎么响应的？
我们有一个对日志进行标记的功能，可以通过这个功能来标记 es 中的那些日志属于错误日志，并且定期轮询匹配，然后把错误日志写入  loki 系统，loki 配置 alertmanage 和我们自己的告警就可以通知了。日志异常和暴增是通过对 prometheus 指标进行监控，上面有提到我们会把这些执行指标存储到 prometheus 中，日志暴增目前只会触发通知和告警，系统层面目前没有处理，这里需要要化，比如加入限流，熔断这种策略

#### Loki 和 ES 这两套日志系统，分别有什么优势和使用场景？你们是如何决定某类日志放哪边存储的？
loki：不对数据内容进行特殊处理，只是一直追加数据到磁盘中，当然它会根据配置合并数据，其主要是靠 label 标签进行索引，加快查询速度，它不太适合对日志进行聚合查询，评分，但是它占用内存小，能满足很多简单日志存储和查询的需求，在我影响中高可用方式好像支持的不太好
es：支持高可用，可存储日志，指标，并且提供的各种不同的检索方式（聚合，分词，评分等），它占用内存大，适合中大型系统日志的存储和检索，并且是 json 的数据结构。


#### 你们怎么保证 Loki 和 ES 中日志数据的一致性？如果同一条日志分别写入两边，如何保证不丢失也不重复？
在我们系统中没有同时写两个地方，我们只是把错误日志从 es 放到 loki 中，然后这个任务是从过分布式调度保证同时只会调度一次，现在没有保障日志不丢失，比如日志还在应用内存中还没写 es 就死机，而且在我们系统的角度也不需要增加额外的成本来保证不丢失，
#### 你提到日志异常暴增时，目前只做了告警，后续要做限流、熔断，你会怎么设计这些功能，能简单讲讲思路吗？

可以预估系统的承载能力，配置同时能执行多少个连接器调度，可以 配置队列的方式进行限流。

可以执行耗时和执行错误数量多的进行标记，后续不执行

#### 假如限流和熔断配置了之后，系统因为熔断不执行了，有些任务会被延迟甚至丢失，你怎么设计保证关键任务不会被丢失？或者说如何优先保障关键任务？

这里可以使用日志 + 队列方式，队列或者可支持优先级，记录执行流水，待恢复后从数据库中重新捞起

#### 请你介绍一下，在过去的项目中，你是如何与产品经理和测试团队协作的？能举个具体例子吗？
具体需要看什么项目中：
在 billing 项目中，产品经理负责整理需求，然后探讨，再就是一起评审需求，
测试：主要是探讨测试点，评审测试，以及测试中的bug疑问

比如我们开始有 ba 会和产品沟通需求，产品整理好后，会记录需求，然后评审需求 大致是这个过程，后面我们 billing 产品没有了，那么 ba 直接和我沟通需求，我形成文档，然后和开发测试评审。

在后面应用平台的时候，我们自己就是产品，开发，测试，运维，部署于一身，多角色。基本不需要和其他团队沟通，主要是公司这个人员配备，人力，物力上的导致的

### “在项目开发过程中，需求变更是常见情况，但往往会导致项目延期。请你结合自己实际经历，谈谈遇到需求变更时，你是如何评估影响、与团队沟通并采取措施，来尽可能减少延期风险的？”
具体看功能模块，以及变更的大小，比如是配置模块就影响会小的，如果是账单模块就影响大，级别比较严重。针对这种级别严重的我会：
1. 找相关同事了解变更的影响有哪些
2. 找测试针对变更重新编写测试用例
然后和开发测试一起评估时间，一起重新对功能和测试和设计进行评审，如果需要人力我会申请加人，但是如果各方面有限，这次变更会和产品商量是否可以考虑放到下次迭代，说明原因，风险等，实时跟进这个需求变更的进度

### 请谈谈你是如何管理和协调跨团队合作的？比如多个团队同时参与一个大型项目时，你如何保证沟通顺畅和项目按时推进？
- 团队内部理清对其他团队的需求
- 团队内部理清其他团队对我们的需求
首先需要弄清楚这个2个不同方面的内容，然后安排优先级。
- 按照优先级去沟通，然后形成备忘，文档，接口，数据结构等记录
- 实时跟进需求，积极推动我们对其他团队的需求，一定要提前推进，比如 5.10 需要一个接口，那么我们需要 5.5 就需要对方提供出来，尽可能让对方快速提供我们需要的东西
- 对外沟通单独安排谨慎的，细心的人。避免不一致和响应错误的信息给其他团队
- 对外提供的东西要准确，全面（比如接口文档，字段，类型等，接口需要测试成功后才能发布给对方）
### 发生资源竞争的时候，要怎么处理
- 先和对方沟通，看其是否存在让步空间
- 是否可以存在轮询使用的空间
- 评估我方的紧张程度
如果发生严重的资源竞争，没有妥协空间，反馈给上级，让上级加资源或者是判断资源优先使用方

### 你们团队遇到过技术方案或者架构设计上的分歧吗？你是怎么推动达成共识的？
- 是否是全新的方案（未使用过）需要做调研和验证
- 已有成熟的方案 我会认真倾听对方的技术方案实现和考虑点（比如 高可用，未来计划，一致性等），然后现有的场景，人力，物力等方面，如果他的确实满足我们现在的情况，我会采用他提供的方案。如果我对对方的方案觉得不合适我会把我的思考点详细的讲述给他听，当然如果我没有办法说服的话，
- 最后我们可以让其他同事参与投票（这个我是打算这样，但是现实可能会存在苦难，其实这是最好的）

### 请问你在工作中是如何管理时间和任务优先级的？遇到紧急任务和正常任务冲突时你怎么处理？
1. 会对任务优先级进行标记，通过截止时间和业务特点（账单就可能比配置重要），利用工具进行提醒（苹果日历）
2. 规划 todo，
3. 优先处理紧急任务，可以选择把正常任务延后或者交与其他同事处理

### 你在团队中遇到过冲突或者分歧吗？你是如何处理的？能举个具体的例子吗？
star表达，情境（Situation）、任务（Task）、行动（Action）、结果（Result）
- 事情原因
- 怎么处理
- 总结
一定要有总结和以后的处理措施

这个比较不好处理，我遇到过  ui 和 前端 冲突，当时她们只是讨论，我在旁边听着她们的讨论进度。后面发现她们的讨论的情绪有点激动了，我就介入了，这个时候第三方介入会让双方很快的冷静下来，接着我会详细了解他们的问题点，根据问题点和他们讨论处理方法，然后一起觉得怎么处理。通过上面的方法很多时候我们都能很好的处理，包括测试和开发，开发和产品直接的问题，以我的经验来说 第三方介入能很好处理这问题，当然介入的要及时，不能让她们吵起来再介入。 
- 最后总结出现这个问题的原因，拆分他们的边界，以后遇到类似情况 我们该怎么处理。

### 你遇到过工作压力很大甚至有点崩溃的情况吗？你是怎么调整的？
遇到过，曾经有遇到一个 问题一直连续 2 天都没有解决好，但是这边下周又需要给客户演示：
- 先暂时放开工作离开办公室到室外放松下（买个饮料，咖啡之类的）
- 然后空闲时间，跑步，散步 等
- 重新梳理工作和问题，分析工作和问题，然后再进入工作
我发现跑步和散步是我非常好的方式，特别是独自一个人，首先会放松，会进入思考的专注模式，容易对梳理工作和问题的原因，以及初步形成应对方案

### 如果领导给了你一个不合理的任务，你会怎么处理？
合理与否是一个角度问题，再领导的角度可能是合理的，在你的角度可能是不合理的。因此我的措施是：
1. 尽力和领导沟通，理解领导意图和目标
2. 制定和调研，以及验证可行性方案
3. 合适的话可以给领导演示效果
4. 在执行过程中，及时反馈进度，听取领导反馈和建议
我首先需要去努力和设法完成领导的这个任务，并且及时反馈任务进度，让领导随时把握任务进度，

### 如果你和领导的意见冲突，你会怎么处理？
保持理性和情绪稳定，不在公共场合如会议上发起质疑，然后私底下与领导进行深入的沟通，充分了解领导的意图，然后再根据领导的意见和目标，调研和制定方案，在执行方案前我会演示下大概的效果，看领导的反馈，在执行方案的过程中，及时给领导反馈进度和遇到的问题，听取领导的意见，如果方案当前无法推进，先反馈领导，让领导知道当前风险，同时准备代替方案